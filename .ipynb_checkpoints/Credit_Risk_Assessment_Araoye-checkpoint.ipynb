{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15a0ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import os\n",
    "from typing import Tuple, Dict, Any\n",
    "from collections import Counter\n",
    "\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution before split:\n",
      "Counter({1: 4913, 0: 985})\n",
      "\n",
      "Training set class distribution:\n",
      "Counter({1: 3930, 0: 788})\n",
      "Test set class distribution:\n",
      "Counter({1: 983, 0: 197})\n",
      "\n",
      "Class distribution after resampling:\n",
      "Counter({0: 3129, 1: 3129})\n",
      "Training model on resampled data with hyperparameter tuning...\n",
      "Applying SMOTETomek resampling...\n",
      "\n",
      "Class distribution after resampling:\n",
      "Counter({0: 3037, 1: 3037})\n",
      "\n",
      "Performing hyperparameter tuning on resampled data...\n",
      "Starting XGBoost hyperparameter tuning...\n",
      "Calculated scale_pos_weight: 1.0\n",
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n"
     ]
    }
   ],
   "source": [
    "class DataPreprocessor(ABC):\n",
    "    \"\"\"Interface for data preprocessing\"\"\"\n",
    "    @abstractmethod\n",
    "    def preprocess(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "class FeatureEngineer(ABC):\n",
    "    \"\"\"Interface for feature engineering\"\"\"\n",
    "    @abstractmethod\n",
    "    def engineer_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "class DataResampler(ABC):\n",
    "    \"\"\"Interface for data resampling\"\"\"\n",
    "    @abstractmethod\n",
    "    def resample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        pass\n",
    "\n",
    "class ModelTrainer(ABC):\n",
    "    \"\"\"Interface for model training\"\"\"\n",
    "    @abstractmethod\n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, X_test: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "class ModelSaver(ABC):\n",
    "    \"\"\"Interface for model persistence\"\"\"\n",
    "    @abstractmethod\n",
    "    def save(self, model, filepath: str):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load(self, filepath: str):\n",
    "        pass\n",
    "\n",
    "class CreditRiskPreprocessor(DataPreprocessor):\n",
    "    def __init__(self, feature_columns=None):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "    \n",
    "    def preprocess(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        processed_data = data.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in processed_data.columns:\n",
    "            if processed_data[col].dtype in ['object']:\n",
    "                processed_data[col].fillna(processed_data[col].mode()[0], inplace=True)\n",
    "            else:\n",
    "                processed_data[col].fillna(processed_data[col].median(), inplace=True)\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        categorical_cols = processed_data.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            processed_data[col] = self.label_encoder.fit_transform(processed_data[col].astype(str))\n",
    "            \n",
    "        #Handle dependents column value 3+\n",
    "        # Replace '3+' with 3 and convert to integer\n",
    "        processed_data['Dependents'] = processed_data['Dependents'].replace('3+', 3).astype(int)\n",
    "        \n",
    "        return processed_data\n",
    "\n",
    "class CreditRiskFeatureEngineer(FeatureEngineer):\n",
    "    def engineer_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        engineered_data = data.copy()\n",
    "        \n",
    "        # Create new features\n",
    "        engineered_data['Total_Income'] = engineered_data['ApplicantIncome'] + engineered_data['CoapplicantIncome']\n",
    "        engineered_data['Loan_Income_Ratio'] = engineered_data['LoanAmount'] / (engineered_data['Total_Income'] + 1)\n",
    "        \n",
    "        # Log transformations\n",
    "        log_cols = ['ApplicantIncome', 'LoanAmount', 'Total_Income']\n",
    "        for col in log_cols:\n",
    "            if col in engineered_data.columns:\n",
    "                engineered_data[f'{col}_Log'] = np.log1p(engineered_data[col])\n",
    "        \n",
    "        # Polynomial features\n",
    "        engineered_data['Income_Squared'] = engineered_data['Total_Income'] ** 2\n",
    "        engineered_data['Loan_Amount_Squared'] = engineered_data['LoanAmount'] ** 2\n",
    "        \n",
    "        return engineered_data\n",
    "\n",
    "class SMOTETomekResampler(DataResampler):\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.resampler = SMOTETomek(random_state=random_state)\n",
    "    \n",
    "    def resample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        return self.resampler.fit_resample(X, y)\n",
    "\n",
    "class SMOTEResampler(DataResampler):\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.resampler = SMOTE(random_state=random_state)\n",
    "    \n",
    "    def resample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        return self.resampler.fit_resample(X, y)\n",
    "\n",
    "class ADASYNResampler(DataResampler):\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.resampler = ADASYN(random_state=random_state)\n",
    "    \n",
    "    def resample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        return self.resampler.fit_resample(X, y)\n",
    "\n",
    "class RandomUndersampler(DataResampler):\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.resampler = RandomUnderSampler(random_state=random_state)\n",
    "    \n",
    "    def resample(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        return self.resampler.fit_resample(X, y)\n",
    "\n",
    "class BaseModelTrainer:\n",
    "    \"\"\"Base class for model trainers\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.cv_results = None\n",
    "    \n",
    "    def get_best_params(self):\n",
    "        return self.best_params\n",
    "    \n",
    "    def get_cv_results(self):\n",
    "        return self.cv_results\n",
    "    \n",
    "class XGBoostModelTrainer(BaseModelTrainer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param_grid = {\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'n_estimators': [100, 200],\n",
    "            'min_child_weight': [1, 3],\n",
    "            'gamma': [0, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'max_delta_step': [1, 2, 4]\n",
    "        }\n",
    "    \n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        # Calculate scale_pos_weight\n",
    "        pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "        \n",
    "        # Update param_grid with scale_pos_weight\n",
    "        full_param_grid = self.param_grid.copy()\n",
    "        full_param_grid['scale_pos_weight'] = [1, pos_weight]\n",
    "        \n",
    "        print(\"Starting XGBoost hyperparameter tuning...\")\n",
    "        print(f\"Calculated scale_pos_weight: {pos_weight}\")\n",
    "        \n",
    "        # Create a validation split from training data\n",
    "        X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        # Define fit parameters including eval set\n",
    "        fit_params = {\n",
    "            'eval_set': [(X_valid_split, y_valid_split)],\n",
    "            'early_stopping_rounds': 10,\n",
    "            'verbose': 0  # Reduce verbosity since we're doing multiple fits\n",
    "        }\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=xgb.XGBClassifier(\n",
    "                random_state=42,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='auc'\n",
    "            ),\n",
    "            param_grid=full_param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            error_score='raise'\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_split, y_train_split)\n",
    "        \n",
    "        self.model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.cv_results = grid_search.cv_results_\n",
    "        \n",
    "        # Final fit on the entire training data with best parameters\n",
    "        print(\"\\nFitting final model with best parameters...\")\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            **self.best_params,\n",
    "            random_state=42,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc'\n",
    "        )\n",
    "        \n",
    "        # Create validation set for final model\n",
    "        X_train_final, X_valid_final, y_train_final, y_valid_final = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        self.model.fit(\n",
    "            X_train_final,\n",
    "            y_train_final,\n",
    "            eval_set=[(X_valid_final, y_valid_final)],\n",
    "#             early_stopping_rounds=10,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nBest parameters found:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test: pd.DataFrame):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self.model.predict(X_test)\n",
    "    \n",
    "    def predict_proba(self, X_test: pd.DataFrame):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self.model.predict_proba(X_test)\n",
    "\n",
    "class RandomForestModelTrainer(BaseModelTrainer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'class_weight': ['balanced', 'balanced_subsample']  # Added to handle class imbalance\n",
    "        }\n",
    "    \n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        # Create validation split\n",
    "        X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(\"Starting Random Forest hyperparameter tuning...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=RandomForestClassifier(random_state=42),\n",
    "            param_grid=self.param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_split, y_train_split)\n",
    "        self.model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.cv_results = grid_search.cv_results_\n",
    "        \n",
    "        # Final fit on entire training data with best parameters\n",
    "        print(\"\\nFitting final Random Forest model with best parameters...\")\n",
    "        self.model = RandomForestClassifier(\n",
    "            **self.best_params,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"\\nBest parameters found:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "class LogisticRegressionModelTrainer(BaseModelTrainer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param_grid = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    \n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=LogisticRegression(random_state=42),\n",
    "            param_grid=self.param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.cv_results = grid_search.cv_results_\n",
    "        return self\n",
    "\n",
    "class JobLibModelSaver(ModelSaver):\n",
    "    def save(self, model, filepath: str):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        joblib.dump(model, filepath)\n",
    "    \n",
    "    def load(self, filepath: str):\n",
    "        return joblib.load(filepath)\n",
    "\n",
    "class CreditRiskAnalyzer:\n",
    "    \"\"\"Main class orchestrating the credit risk analysis process\"\"\"\n",
    "    def __init__(self, \n",
    "                 preprocessor: DataPreprocessor,\n",
    "                 feature_engineer: FeatureEngineer,\n",
    "                 model_trainer: ModelTrainer,\n",
    "                 model_saver: ModelSaver,\n",
    "                 resampler: DataResampler = None):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.feature_engineer = feature_engineer\n",
    "        self.model_trainer = model_trainer\n",
    "        self.model_saver = model_saver\n",
    "        self.resampler = resampler\n",
    "        self.feature_columns = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "    def prepare_data(self, data: pd.DataFrame, target_column: str='Loan_Status', test_size: float=0.2):\n",
    "        \"\"\"Prepare data with enhanced feature selection and validation\"\"\"\n",
    "        # Preprocess\n",
    "        processed_data = self.preprocessor.preprocess(data)\n",
    "\n",
    "        # Engineer features\n",
    "        engineered_data = self.feature_engineer.engineer_features(processed_data)\n",
    "\n",
    "        # Validate features exist\n",
    "        required_features = [\n",
    "            'Loan_Amount_Term', 'Credit_History', 'Loan_Income_Ratio',\n",
    "            'LoanAmount_Log', 'Total_Income_Log',\n",
    "            'Income_Squared', 'Loan_Amount_Squared', 'Married', 'Dependents'\n",
    "        ]\n",
    "\n",
    "        missing_features = [col for col in required_features if col not in engineered_data.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "\n",
    "        # Define feature columns\n",
    "        self.feature_columns = [col for col in required_features if col in engineered_data.columns]\n",
    "\n",
    "        # Prepare features and target\n",
    "        X = engineered_data[self.feature_columns]\n",
    "        y = engineered_data[target_column]\n",
    "\n",
    "        # Print class distribution before split\n",
    "        print(f\"\\nClass distribution before split:\\n{Counter(y)}\")\n",
    "\n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Print class distribution after split\n",
    "        print(f\"\\nTraining set class distribution:\\n{Counter(self.y_train)}\")\n",
    "        print(f\"Test set class distribution:\\n{Counter(self.y_test)}\")\n",
    "\n",
    "        # Apply resampling if resampler is provided\n",
    "        if self.resampler:\n",
    "            self.X_train, self.y_train = self.resampler.resample(self.X_train, self.y_train)\n",
    "            print(f\"\\nClass distribution after resampling:\\n{Counter(self.y_train)}\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def compare_resampling_techniques(self) -> pd.DataFrame:\n",
    "        \"\"\"Compare different resampling techniques with various models and log class distributions\"\"\"\n",
    "        if self.X_train is None or self.y_train is None:\n",
    "            raise ValueError(\"Data not prepared yet. Call prepare_data first.\")\n",
    "\n",
    "        # Log original class distribution\n",
    "        original_dist = Counter(self.y_train)\n",
    "        print(f\"Original class distribution:\\n{original_dist}\")\n",
    "\n",
    "        resamplers = {\n",
    "            'No Resampling': None,\n",
    "            'SMOTE': SMOTEResampler(),\n",
    "            'ADASYN': ADASYNResampler(),\n",
    "            'RandomUndersampler': RandomUndersampler(),\n",
    "            'SMOTETomek': SMOTETomekResampler()\n",
    "        }\n",
    "\n",
    "        models = {\n",
    "            'XGBoost': xgb.XGBClassifier(random_state=42),\n",
    "            'RandomForest': RandomForestClassifier(random_state=42),\n",
    "            'LogisticRegression': LogisticRegression(random_state=42)\n",
    "        }\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for resampler_name, resampler in resamplers.items():\n",
    "            X_train, y_train = self.X_train.copy(), self.y_train.copy()\n",
    "\n",
    "            if resampler:\n",
    "                X_train, y_train = resampler.resample(X_train, y_train)\n",
    "                resampled_dist = Counter(y_train)\n",
    "                print(f\"\\n{resampler_name} class distribution:\\n{resampled_dist}\")\n",
    "\n",
    "            for model_name, model in models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(self.X_test)\n",
    "\n",
    "                # Handle zero division in metrics\n",
    "                results.append({\n",
    "                    'Resampling': resampler_name,\n",
    "                    'Model': model_name,\n",
    "                    'Accuracy': model.score(self.X_test, self.y_test),\n",
    "                    'F1 Score': f1_score(self.y_test, y_pred, zero_division=0),\n",
    "                    'Precision': precision_score(self.y_test, y_pred, zero_division=0),\n",
    "                    'Recall': recall_score(self.y_test, y_pred, zero_division=0),\n",
    "                    'Samples After Resampling': len(y_train)\n",
    "                })\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        self._plot_resampling_comparison(results_df)\n",
    "        return results_df\n",
    "    \n",
    "    def _plot_resampling_comparison(self, results_df: pd.DataFrame):\n",
    "        \"\"\"Plot resampling comparison results\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        metrics = ['F1 Score', 'Precision', 'Recall', 'Accuracy']\n",
    "        for i, metric in enumerate(metrics, 1):\n",
    "            plt.subplot(2, 2, i)\n",
    "            sns.barplot(x='Resampling', y=metric, hue='Model', data=results_df)\n",
    "            plt.title(f'{metric} Comparison')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"Train the model on resampled data with hyperparameter tuning\"\"\"\n",
    "        if self.X_train is None or self.y_train is None:\n",
    "            raise ValueError(\"Data not prepared yet. Call prepare_data first.\")\n",
    "        \n",
    "        # Apply resampling first\n",
    "        print(\"Applying SMOTETomek resampling...\")\n",
    "        X_resampled, y_resampled = self.resampler.resample(self.X_train, self.y_train)\n",
    "        \n",
    "        # Print class distribution after resampling\n",
    "        print(\"\\nClass distribution after resampling:\")\n",
    "        print(Counter(y_resampled))\n",
    "        \n",
    "        # Train model with hyperparameter tuning on resampled data\n",
    "        print(\"\\nPerforming hyperparameter tuning on resampled data...\")\n",
    "        self.model_trainer.train(X_resampled, y_resampled)\n",
    "        \n",
    "        # Print best parameters\n",
    "        print(\"\\nBest parameters found:\")\n",
    "        print(self.model_trainer.get_best_params())\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = self.model_trainer.predict(self.X_test)\n",
    "        metrics = {\n",
    "            'Accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'F1 Score': f1_score(self.y_test, y_pred),\n",
    "            'Precision': precision_score(self.y_test, y_pred),\n",
    "            'Recall': recall_score(self.y_test, y_pred)\n",
    "        }\n",
    "        print(\"\\nTest set performance metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the trained model\"\"\"\n",
    "        if not hasattr(self.model_trainer, 'model') or self.model_trainer.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "\n",
    "        y_pred = self.model_trainer.predict(self.X_test)\n",
    "        y_pred_proba = self.model_trainer.model.predict_proba(self.X_test)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': self.model_trainer.model.score(self.X_test, self.y_test),\n",
    "            'f1': f1_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred),\n",
    "            'recall': recall_score(self.y_test, y_pred)\n",
    "        }\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.model_trainer.model.classes_)\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "        # ROC Curve and AUC\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        # Add ROC-AUC to metrics\n",
    "        metrics['roc_auc'] = roc_auc\n",
    "\n",
    "        # Feature Importance using SHAP\n",
    "        import shap\n",
    "        explainer = shap.TreeExplainer(self.model_trainer.model)\n",
    "        shap_values = explainer.shap_values(self.X_test)\n",
    "\n",
    "        # Summary plot\n",
    "        shap.summary_plot(shap_values, self.X_test, plot_type=\"bar\")\n",
    "        plt.title('Feature Importance (SHAP)')\n",
    "        plt.show()\n",
    "\n",
    "        # Feature Interpretation using LIME\n",
    "        from lime.lime_tabular import LimeTabularExplainer\n",
    "        lime_explainer = LimeTabularExplainer(\n",
    "            training_data=self.X_test.values,\n",
    "            feature_names=self.X_test.columns,\n",
    "            class_names=self.model_trainer.model.classes_,\n",
    "            mode='classification'\n",
    "        )\n",
    "        # Explain a single prediction (example: first instance in test set)\n",
    "        i = 0 \n",
    "        lime_exp = lime_explainer.explain_instance(\n",
    "            data_row=self.X_test.iloc[i].values,\n",
    "            predict_fn=self.model_trainer.model.predict_proba\n",
    "        )\n",
    "        lime_exp.show_in_notebook(show_table=True)\n",
    "        lime_exp.save_to_file('lime_explanation.html')\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if not hasattr(self.model_trainer, 'model') or self.model_trainer.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Call train_model() first.\")\n",
    "        \n",
    "        print(f\"Saving model to {filepath}...\")\n",
    "        self.model_saver.save(self.model_trainer.model, filepath)\n",
    "        print(\"Model saved successfully!\")\n",
    "        return self\n",
    "    \n",
    "    def load_model(self, filepath: str):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        loaded_model = self.model_saver.load(filepath)\n",
    "        self.model_trainer.model = loaded_model\n",
    "        return self\n",
    "    \n",
    "    def predict_new_data(self, new_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        # Preprocess new data\n",
    "        processed_data = self.preprocessor.preprocess(new_data)\n",
    "        engineered_data = self.feature_engineer.engineer_features(processed_data)\n",
    "        \n",
    "        # Select features\n",
    "        X_new = engineered_data[self.feature_columns]\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model_trainer.predict(X_new)\n",
    "        probabilities = self.model_trainer.predict_proba(X_new)[:, 1]\n",
    "        \n",
    "        # Prepare results\n",
    "        results = new_data.copy()\n",
    "        results['Predicted_Loan_Status'] = predictions\n",
    "        results['Loan_Approval_Probability'] = probabilities\n",
    "        \n",
    "        return results\n",
    "    \n",
    "def main():\n",
    "    # Initialize components\n",
    "    preprocessor = CreditRiskPreprocessor()\n",
    "    feature_engineer = CreditRiskFeatureEngineer()\n",
    "    resampler = SMOTETomekResampler()\n",
    "    xgb_trainer = XGBoostModelTrainer()\n",
    "    rf_trainer = RandomForestModelTrainer()\n",
    "    model_saver = JobLibModelSaver()\n",
    "\n",
    "    # Create two analyzers, one for each model\n",
    "    xgb_analyzer = CreditRiskAnalyzer(\n",
    "        preprocessor=preprocessor,\n",
    "        feature_engineer=feature_engineer,\n",
    "        model_trainer=xgb_trainer,\n",
    "        model_saver=model_saver,\n",
    "        resampler=resampler\n",
    "    )\n",
    "\n",
    "    rf_analyzer = CreditRiskAnalyzer(\n",
    "        preprocessor=preprocessor,\n",
    "        feature_engineer=feature_engineer,\n",
    "        model_trainer=rf_trainer,\n",
    "        model_saver=model_saver,\n",
    "        resampler=resampler\n",
    "    )\n",
    "\n",
    "    # Load and prepare training data\n",
    "    train_data = pd.read_csv('credit-worthiness-prediction/train.csv')\n",
    "\n",
    "    # Train and evaluate XGBoost\n",
    "    print(\"\\n=== Training and Evaluating XGBoost Model ===\")\n",
    "    xgb_analyzer.prepare_data(train_data)\n",
    "    xgb_analyzer.train_model()\n",
    "    xgb_metrics = xgb_analyzer.evaluate_model()\n",
    "    print(\"\\nXGBoost Model Metrics:\")\n",
    "    for metric, value in xgb_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Train and evaluate Random Forest\n",
    "    print(\"\\n=== Training and Evaluating Random Forest Model ===\")\n",
    "    rf_analyzer.prepare_data(train_data)\n",
    "    rf_analyzer.train_model()\n",
    "    rf_metrics = rf_analyzer.evaluate_model()\n",
    "    print(\"\\nRandom Forest Model Metrics:\")\n",
    "    for metric, value in rf_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Save both models\n",
    "    xgb_analyzer.save_model('models/xgboost_model.joblib')\n",
    "    rf_analyzer.save_model('models/random_forest_model.joblib')\n",
    "\n",
    "    # Make predictions on test data\n",
    "    test_data = pd.read_csv('credit-worthiness-prediction/test.csv')\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    xgb_predictions = xgb_analyzer.predict_new_data(test_data)\n",
    "    rf_predictions = rf_analyzer.predict_new_data(test_data)\n",
    "\n",
    "    # Combine predictions\n",
    "    combined_predictions = pd.DataFrame({\n",
    "        'xgboost_predictions': xgb_predictions['loan_status'],\n",
    "        'random_forest_predictions': rf_predictions['loan_status']\n",
    "    })\n",
    "    \n",
    "    combined_predictions.to_csv('predicted_loan_status.csv', index=False)\n",
    "    print(\"\\nPredictions saved to 'predicted_loan_status.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273a1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
